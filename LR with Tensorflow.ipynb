{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Algorithm with Tensorflow\n",
    "\n",
    "This is a simple logistic regression algorithm implemented using Tensorflow.\n",
    "First, we will import the libraries needed for the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will generate some data for the LR algorithm to work on. The first dataset consists of vectors in the form of [0, 1, 2,..., 19], normalized to the range of (0, 1), and then contaminated by random noise. The second dataset is made up of pure random noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate two datasets\n",
    "datagroup1 = (np.tile(np.arange(20),(1000,1))/19 + np.random.random((1000,20)))/2\n",
    "datagroup2 = np.random.random((1000,20))\n",
    "datagroup1_labeled = np.concatenate((datagroup1, np.ones((1000,1))), axis=1) \n",
    "datagroup2_labeled = np.concatenate((datagroup2, np.zeros((1000,1))), axis=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we concatenate the two dataset and shuffle them before feeding them to the LR algorithm. We also need to do some reshaping to make sure the data input is of the right dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldata = np.concatenate((datagroup1_labeled, datagroup2_labeled), axis=0)\n",
    "np.random.shuffle(alldata)\n",
    "# The first 1500 entries are used as train data\n",
    "x_train = alldata[0:1500,0:20]\n",
    "y_train = alldata[0:1500,20]\n",
    "# The next 500 entries are used as test data\n",
    "x_test = alldata[1500:2000,0:20]\n",
    "y_test = alldata[1500:2000,20]\n",
    "# Reshape labels to meet Tensorflow requirements\n",
    "y_train = np.reshape(y_train, [y_train.shape[0], 1])\n",
    "y_test = np.reshape(y_test, [y_test.shape[0], 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to perform one-hot encoding to the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'oneHot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-6c56485d2a1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Encoding labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moneHot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneHot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moneHot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moneHot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'oneHot' is not defined"
     ]
    }
   ],
   "source": [
    "# Creating the One Hot Encoder \n",
    "oneHot = OneHotEncoder() \n",
    "# Encoding labels\n",
    "oneHot.fit(y_train) \n",
    "y_train = oneHot.transform(y_train).toarray() \n",
    "oneHot.fit(y_test) \n",
    "y_test = oneHot.transform(y_test).toarray() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code to implement a LR model in Tensorflow is much chunkier than that in Keras. We first need to initialize placeholders for both input data and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create placeholders the size of data and label\n",
    "X = tf.placeholder(tf.float32, [None, 20]) \n",
    "Y = tf.placeholder(tf.float32, [None, 2]) \n",
    "# Trainable variable weights \n",
    "W = tf.Variable(tf.zeros([20, 2])) \n",
    "# Trainable variable bias \n",
    "b = tf.Variable(tf.zeros([2])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to define some hyperparameters (learning rate and the number of epochs to train)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "lr, epochs = 0.0015, 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Tensorflow, we need to explictly define the math model used for LR, as well as the cost fundtion we wish to minimize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow implementation of LR\n",
    "Y_hat = tf.nn.sigmoid(tf.add(tf.matmul(X, W), b)) \n",
    "  \n",
    "# Sigmoid Cross Entropy Cost Function \n",
    "cost = tf.nn.sigmoid_cross_entropy_with_logits(logits = Y_hat, labels = Y) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then have to specify the optimizer, and initialize variables for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Descent Optimizer \n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lr).minimize(cost) \n",
    "  \n",
    "# Global Variables Initializer \n",
    "init = tf.global_variables_initializer() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow implementations goes by the name \"sessions\", we need to create a session for the training and test data, and display the training accuracy at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the Tensorflow Session \n",
    "with tf.Session() as sess: \n",
    "      \n",
    "    # Initializing the Variables \n",
    "    sess.run(init) \n",
    "      \n",
    "    # Lists for storing the changing Cost and Accuracy in every Epoch \n",
    "    cost_history, accuracy_history = [], [] \n",
    "      \n",
    "    # Iterating through all the epochs \n",
    "    for epoch in range(epochs): \n",
    "        cost_per_epoch = 0\n",
    "          \n",
    "        # Running the Optimizer and cost \n",
    "        _, c = sess.run([optimizer, cost], feed_dict = {X : x_train, Y : y_train}) \n",
    "          \n",
    "        # Calculating accuracy on current Epoch \n",
    "        correct_prediction = tf.equal(tf.argmax(Y_hat, 1), tf.argmax(Y, 1)) \n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "          \n",
    "        # Storing Cost and Accuracy to the history \n",
    "        cost_history.append(sum(sum(c))) \n",
    "        accuracy_history.append(accuracy.eval({X : x_train, Y : y_train}) * 100) \n",
    "          \n",
    "        # Displaying result on current Epoch \n",
    "        if epoch % 10 == 0 and epoch != 0: \n",
    "            print(\"Epoch \" + str(epoch) + \" Accuracy: \" + str(\"%.1f\" % accuracy_history[-1])  + \"%\") \n",
    "      \n",
    "    Weight = sess.run(W) # Optimized Weight \n",
    "    Bias = sess.run(b)   # Optimized Bias \n",
    "      \n",
    "    # Final Accuracy \n",
    "    correct_prediction = tf.equal(tf.argmax(Y_hat, 1), tf.argmax(Y, 1)) \n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32)) \n",
    "    trainacc = accuracy.eval({X : x_train, Y : y_train}) * 100\n",
    "    testacc = accuracy.eval({X : x_test, Y : y_test}) * 100\n",
    "    print(\"Train Accuracy:\", str(\"%.1f\" % trainacc), \"%\")\n",
    "    print(\"Test Accuracy:\", str(\"%.1f\" % testacc), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
